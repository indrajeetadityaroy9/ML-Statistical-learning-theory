\documentclass[10pt]{article}
\usepackage[colorlinks,urlcolor=blue]{hyperref}
\usepackage[osf]{mathpazo}
\usepackage{amsthm,amsmath,amsfonts,graphicx}
\usepackage{latexsym}
\usepackage{subfig}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{fullpage}
\usepackage{color}
\usepackage{mathtools}

\definecolor{mdb}{rgb}{0.3,0.02,0.02} 
\definecolor{cit}{rgb}{0.05,0.2,0.45} 

\newcommand{\handout}{
   \renewcommand{\thepage}{H\hnumber-\arabic{page}}
   \noindent
   \begin{center}
      \vbox{
    \hbox to \columnwidth {\sc{\course} --- \prof \hfill}
    \vspace{-2mm}
    \hbox to \columnwidth {\sc due \MakeLowercase{\duedate} \duelocation\hfill {\Huge\color{mdb}H\hnumber.\yourname}}
      }
   \end{center}
   \vspace*{2mm}
}
\newcommand{\solution}[1]{\medskip\noindent{\color{cit}\textbf{Solution:}} #1}

\newcommand{\bit}[1]{\{0,1\}^{ #1 }}

\theoremstyle{definition}
\newtheorem{problem}{\sc\color{cit}problem}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}

\def\R{\mathbb{R}}%
\def\softmax{\mathrm{softmax}}%

\begin{document}


\begin{problem}[Attention, 40 pts]
In this problem, we will explore the expressiveness of the attention
mechanism by trying to achieve different goals using attention. Recall
that we have $n$ keys $k_{1},\ldots,k_{n}\in\R^{d}$ and $n$ corresponding
values $v_{1},\ldots,v_{n}\in\R^{d}$. On a query vector $q\in\R^{d}$,
we have the attention score $s_{i}=\left\langle k_{i},q\right\rangle $,
the attention weight $\alpha_{i}=\frac{\exp(s_{i})}{\sum_{j=1}^{n}\exp(s_{j})}$
and the output is $\sum_{i=1}^{n}\alpha_{i}v_{i}$.

\begin{enumerate}[label = (\alph*)]
\item (10 pts) \textbf{Copying a distinct value}. In this part, we will show that
attention can pick out a key that is very different from the rest.
Assume that due to normalization $\left\Vert k_{i}\right\Vert =1\ \forall i$.
Consider the case when one key $k_{1}$ is far from all other keys:
$\left\Vert k_{1}-k_{i}\right\Vert \ge1\ \forall i\ne1$. Fix a constant
$\epsilon\in(0,1)$. Give a query $q$ of length at most $2\ln(n/\epsilon)$
such that the attention weight $\alpha_{1}\ge1/(1+\epsilon)$ and
justify your answer.

\item (10 pts)  \textbf{Averaging two values}. In this part, we will show that attention
can also combine information from two different values. Again assume
that due to normalization $\left\Vert k_{i}\right\Vert =1\ \forall i$.
Furthermore, assume that the keys are all orthogonal $\left\langle k_{i},k_{j}\right\rangle =0\ \forall i\ne j$.
Give a query $q$ of length at most $2\sqrt{2}\ln(n/\epsilon)$ such
that the attention weights $\alpha_{1},\alpha_{2}\ge\frac{1}{2+\epsilon}$
and justify your answer.

\item (10 pts) \textbf{Averaging when dealing with noise}. In this part, we will
show that single-headed attention might have trouble when the keys
are noisy. Assume that the keys are all orthogonal $\left\langle k_{i},k_{j}\right\rangle =0\ \forall i\ne j$.
We also assume that $\left\Vert k_{i}\right\Vert =1\ \forall i\ne1$.
The difference compared with (b) is that the first key is chosen randomly:
$k_{1}=c\cdot u$ where $u$ is a unit vector and $c$ is sampled
uniformly from $\{1/2,3/2\}$. Describe how the attention weights
$\alpha_{1}$ and $\alpha_{2}$ differ from (b) when you use the query
vector $q=\ln(n/\epsilon)(u+k_{2})$. Justify your answer.

\item (10 pts) \textbf{Averaging using multi-headed attention}. In this part, we
see if multi-headed attention helps. The heads share the same keys
and values. Consider the same example in (c) but now we have two (potentially
different) queries $q_{1}$ and $q_{2}$. For each query $q_{i}$
we obtain output $o_{i}$. The final output is the average $(o_{1}+o_{2})/2$.
Give two queries $q_{1}$ and $q_{2}$ with length at most $2\ln(n/\epsilon)$
so that the final output $(o_{1}+o_{2})/2$ is close to $(v_{1}+v_{2})/2$
(what we obtained in part (b) when there is no noise).
\end{enumerate}
\end{problem}

\begin{problem}[Position encoding, 30 pts]
In this problem we will explore the need for positional encodings
and their limitations. Consider a simple network with one attention
layer and one feed-forward layer. The input is a matrix $X\in\R^{n\times d}$
consisting of $n$ input tokens where each token is represented by
a vector in $\R^{d}$. The input is transformed into keys, values,
and queries using weight matrices $W_{K},W_{V},W_{Q}\in\R^{d\times d}$:
\[
Q=XW_{Q},K=XW_{K},V=XW_{V},O=\softmax\left(\frac{QK^{\top}}{\sqrt{d}}\right)V
\]
The output $O\in\R^{n\times d}$ is then fed to a feed-forward layer:
\[
Z=\sigma(OW_{1}+1b_{1}^{\top})W_{2}+1b_{2}^{\top}
\]
where $W_{1},W_{2}\in\R^{d\times d}$ and $b_{1},b_{2}\in\R^{d\times1}$
are weights and biases and $1\in\R^{n\times1}$ is a vector of ones.
The activation $\sigma$ is applied element-wise. The final output
is $Z\in\R^{n\times d}$.

\begin{enumerate}[label=(\alph*)]
\item (25 pts)  Suppose we permute the input tokens by applying a permutation matrix
$P\in\R^{n\times n}$: $X^{perm}=PX$. Note that $P$ is a matrix
with exactly one non-zero entry per row and per column and all the
non-zeroes are ones. An important property of $P$ is that $PP^{\top}=P^{\top}P=I$.
Let $Z^{perm}$ be the output for input $X^{perm}$. Show that $Z^{perm}=PZ$.
Hint: verify that for any permutation matrix $P$ and any matrix $A$,
we have $\softmax(PAP^{\top})=P\softmax(A)P^{\top},\sigma(PA)=P\sigma(A)$.
\item (5 pts) Suppose we use the following positional encoding. Let $\Phi\in\R^{n\times d}$
be the matrix with 
\begin{align*}
\Phi_{t,2i} & =\sin\left(t/10000^{2i/d}\right)\\
\Phi_{t,2i+1} & =\cos\left(t/10000^{2i/d}\right)
\end{align*}
 for $t\in\{0,1,\ldots,n\}$, $i\in\{0,1,\ldots,d/2-1\}$. The positional
encoding is then added to the input embedding: $X^{pos}=X+\Phi$.
Show that for any $t\ne t'$, the rows $t$ and $t'$ of $\Phi$ are
different vectors.
\end{enumerate}
\end{problem}

\begin{problem}[SVD, 30 pts]
In this problem, we will derive some theoretical guarantees for approximating
a given matrix using a low rank matrix. Consider a matrix $A\in\R^{n\times d}$
with $n\ge d$. The Frobenius norm of $A$ is defined as $\left\Vert A\right\Vert _{F}=\sqrt{\sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}^{2}}$.
Let $\sigma_{1}\ge\sigma_{2}\ge\ldots\ge\sigma_{d}$ be the singular
values of $A$. Let $A=U\Sigma V^{\top}$be the SVD of $A$.

\begin{enumerate}[label=(\alph*)]
\item  (12 pts) Show that $\sum_{i=1}^{d}\sigma_{i}^{2}=\left\Vert A\right\Vert _{F}^{2}$.
Hint: note that $\left\Vert A\right\Vert _{F}^{2}=trace\left(A^{\top}A\right)$.
\item (6 pts) Show that $\sigma_{k}^{2}\le\left\Vert A\right\Vert _{F}^{2}/k$.
\item (12 pts) Show that there exists a matrix $B$ of rank $k-1$ such that the
maximum singular value of $A-B$ is at most $\left\Vert A\right\Vert _{F}/\sqrt{k}$.
\end{enumerate}
\end{problem}

\end{document}
